{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "12573c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d115e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置信息\n",
    "CONFIG = {\n",
    "    \"base_url\": \"https://api.zhizengzeng.com/v1/\",\n",
    "    \"api_key\": \"sk-zk20f741becece1c055c848225093b2e458662329a0f1016\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5448baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型列表\n",
    "MODELS = [\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"claude-sonnet-4-20250514\", \n",
    "    \"gemini-2.5-flash\",\n",
    "    \"grok-3-mini\",\n",
    "    \"deepseek-chat\",\n",
    "    \"qwen3-coder-plus\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5afe68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    生成用于大模型的英文提示词\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    **Background:** Ambiguity is the phenomenon that occurs when a sentence can be interpreted in more than one way. \n",
    "    It is defined as the ambiguity that leads to multiple interpretations by human readers, be it in the form of their disagreement on the interpretation or acknowledgment of multiple possible readings. \n",
    "    Having multiple interpretations might lead to misunderstanding the requirements, and thus cause significant implications on the success of the development process.\n",
    "    \n",
    "    Referential ambiguity (also called anaphora ambiguity) occurs when a pronoun is preceded by multiple antecedents. For example, in the following sentence, the pronoun \"it\" has three syntactically valid antecedents (in bold), although only the latter two are semantically valid:\n",
    "    \"The **procedure** shall convert the **24 bit image** to an **8 bit image**, then display **it** in a dynamic window.\"\n",
    "    \n",
    "    **Role:** You are a professional natural language processing expert specializing in handling referential ambiguity (anaphora ambiguity) in software requirements specifications.\n",
    "\n",
    "    **Objectives:** Your task is to analyze the following sentence and complete the following tasks:\n",
    "    \n",
    "    1. Detect whether there is referential ambiguity in the sentence (\"Detected as\"):\n",
    "       - If the pronoun in the sentence is ambiguous (i.e., there are multiple possible antecedents), mark it as \"nocuous\"\n",
    "       - If the pronoun in the sentence is not ambiguous (there is only one clear antecedent), mark it as \"innocuous\"\n",
    "    \n",
    "    2. Perform anaphora resolution for the pronoun in the sentence (\"Resolved as\"):\n",
    "       - If the sentence is marked as \"nocuous\", indicate the most likely antecedent of the pronoun\n",
    "       - If the sentence is marked as \"innocuous\", indicate the antecedent of the pronoun\n",
    "       - If unable to determine, return \"unknown\"\n",
    "    \n",
    "    Sentence: {sentence}\n",
    "    \n",
    "    Please output your analysis result in the following JSON format:\n",
    "    {{\n",
    "        \"Detected as\": \"...\",\n",
    "        \"Resolved as\": \"...\"\n",
    "    }}\n",
    "    \n",
    "    Notes:\n",
    "    - The value of \"Detected as\" can only be \"nocuous\" or \"innocuous\"\n",
    "    - The value of \"Resolved as\" should be the specific content the pronoun refers to or \"unknown\"\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ddcfaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt: str, model: str) -> Dict:\n",
    "    \"\"\"\n",
    "    调用大模型API使用OpenAI包\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = OpenAI(api_key=CONFIG[\"api_key\"], base_url=CONFIG[\"base_url\"])\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # 尝试解析JSON格式的响应\n",
    "        match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group()\n",
    "            parsed_result = json.loads(json_str)\n",
    "            return parsed_result\n",
    "        else:\n",
    "            # 如果无法解析JSON，返回默认值\n",
    "            return {\n",
    "                \"Detected as\": \"nocuous\",\n",
    "                \"Resolved as\": \"unknown\"\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"API call failed ({model}): {str(e)}\")\n",
    "        return {\n",
    "            \"Detected as\": \"nocuous\",\n",
    "            \"Resolved as\": \"unknown\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "489625ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_detection(y_true: List[str], y_pred: List[str]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    计算检测任务的精确率和召回率\n",
    "    \"\"\"\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        true_is_nocuous = true_label == \"nocuous\"\n",
    "        pred_is_nocuous = pred_label == \"nocuous\"\n",
    "        \n",
    "        if true_is_nocuous and pred_is_nocuous:\n",
    "            true_positives += 1\n",
    "        elif not true_is_nocuous and pred_is_nocuous:\n",
    "            false_positives += 1\n",
    "        elif true_is_nocuous and not pred_is_nocuous:\n",
    "            false_negatives += 1\n",
    "    \n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dda16bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_resolution(true_labels: List[str], true_disambiguations: List[str], \n",
    "                      pred_labels: List[str], pred_disambiguations: List[str]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    计算消歧任务的精确率和召回率\n",
    "    \"\"\"\n",
    "    correct_resolutions = 0\n",
    "    attempted_resolutions = 0\n",
    "    total_unambiguous = 0\n",
    "    \n",
    "    for i in range(len(true_labels)):\n",
    "        true_label = true_labels[i]\n",
    "        pred_label = pred_labels[i]\n",
    "        true_disambiguation = true_disambiguations[i]\n",
    "        pred_disambiguation = pred_disambiguations[i]\n",
    "        \n",
    "        # 统计尝试消歧的数量\n",
    "        if pred_label == \"nocuous\" and pred_disambiguation != \"unknown\":\n",
    "            attempted_resolutions += 1\n",
    "            # 检查预测结果是否正确（支持部分匹配）\n",
    "            if true_label == \"nocuous\":\n",
    "                # 完全匹配\n",
    "                if true_disambiguation == pred_disambiguation:\n",
    "                    correct_resolutions += 1\n",
    "                # 部分匹配：预测结果包含在真实结果中，或真实结果包含在预测结果中\n",
    "                elif true_disambiguation in pred_disambiguation or pred_disambiguation in true_disambiguation:\n",
    "                    correct_resolutions += 1\n",
    "        \n",
    "        # 统计真实无歧义的数量\n",
    "        if true_label == \"nocuous\":\n",
    "            total_unambiguous += 1\n",
    "    \n",
    "    precision = correct_resolutions / attempted_resolutions if attempted_resolutions > 0 else 0\n",
    "    recall = correct_resolutions / total_unambiguous if total_unambiguous > 0 else 0\n",
    "    \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b9e462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(df: pd.DataFrame, model: str) -> Dict:\n",
    "    \"\"\"\n",
    "    处理数据集并评估模型性能\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"model\": model,\n",
    "        \"predictions\": [],\n",
    "        \"metrics\": {}\n",
    "    }\n",
    "    \n",
    "    print(f\"Processing model {model}...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        sentence = row['Sentence']\n",
    "        true_detected = row['Detected as']\n",
    "        true_disambiguated = row['Resolved as']\n",
    "        \n",
    "        prompt = get_prompt(sentence)\n",
    "        prediction = call_llm(prompt, model)\n",
    "        \n",
    "        # 确保预测结果格式正确\n",
    "        if \"Detected as\" not in prediction:\n",
    "            prediction[\"Detected as\"] = \"nocuous\"\n",
    "        if \"Resolved as\" not in prediction:\n",
    "            prediction[\"Resolved as\"] = \"unknown\"\n",
    "        \n",
    "        results[\"predictions\"].append({\n",
    "            \"id\": row['ID'],\n",
    "            \"sentence\": sentence,\n",
    "            \"true_detected\": true_detected,\n",
    "            \"true_disambiguated\": true_disambiguated,\n",
    "            \"pred_detected\": prediction[\"Detected as\"],\n",
    "            \"pred_disambiguated\": prediction[\"Resolved as\"]\n",
    "        })\n",
    "        \n",
    "        # 添加进度信息\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(df)} samples\")\n",
    "    \n",
    "    # 计算指标\n",
    "    true_detected_list = [item[\"true_detected\"] for item in results[\"predictions\"]]\n",
    "    pred_detected_list = [item[\"pred_detected\"] for item in results[\"predictions\"]]\n",
    "    true_disambiguated_list = [item[\"true_disambiguated\"] for item in results[\"predictions\"]]\n",
    "    pred_disambiguated_list = [item[\"pred_disambiguated\"] for item in results[\"predictions\"]]\n",
    "    \n",
    "    # 检测任务指标\n",
    "    det_precision, det_recall = evaluate_detection(true_detected_list, pred_detected_list)\n",
    "    det_f2 = (1 + 2*2) * det_precision * det_recall / (2*2 * det_precision + det_recall) if (2*2 * det_precision + det_recall) > 0 else 0\n",
    "    \n",
    "    # 消歧任务指标\n",
    "    res_precision, res_recall = evaluate_resolution(\n",
    "        true_detected_list, true_disambiguated_list,\n",
    "        pred_detected_list, pred_disambiguated_list\n",
    "    )\n",
    "    res_f2 = (1 + 2*2) * res_precision * res_recall / (2*2 * res_precision + res_recall) if (2*2 * res_precision + res_recall) > 0 else 0\n",
    "    \n",
    "    results[\"metrics\"] = {\n",
    "        \"detection\": {\n",
    "            \"precision\": det_precision,\n",
    "            \"recall\": det_recall,\n",
    "            \"f2\": det_f2\n",
    "        },\n",
    "        \"resolution\": {\n",
    "            \"precision\": res_precision,\n",
    "            \"recall\": res_recall,\n",
    "            \"f2\": res_f2\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"  Detection - Precision: {det_precision:.3f}, Recall: {det_recall:.3f}, F2: {det_f2:.3f}\")\n",
    "    print(f\"  Resolution - Precision: {res_precision:.3f}, Recall: {res_recall:.3f}, F2: {res_f2:.3f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "920622f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualization(results: List[Dict]):\n",
    "    \"\"\"\n",
    "    创建可视化图表\n",
    "    \"\"\"\n",
    "    models = [r[\"model\"] for r in results]\n",
    "    \n",
    "    # 提取指标数据\n",
    "    det_precisions = [r[\"metrics\"][\"detection\"][\"precision\"] for r in results]\n",
    "    det_recalls = [r[\"metrics\"][\"detection\"][\"recall\"] for r in results]\n",
    "    det_f2s = [r[\"metrics\"][\"detection\"][\"f2\"] for r in results]\n",
    "    \n",
    "    res_precisions = [r[\"metrics\"][\"resolution\"][\"precision\"] for r in results]\n",
    "    res_recalls = [r[\"metrics\"][\"resolution\"][\"recall\"] for r in results]\n",
    "    res_f2s = [r[\"metrics\"][\"resolution\"][\"f2\"] for r in results]\n",
    "    \n",
    "    # 创建图表\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    # 检测任务精确率和召回率\n",
    "    axes[0, 0].bar(x - width/2, det_precisions, width, label='Precision', alpha=0.8)\n",
    "    axes[0, 0].bar(x + width/2, det_recalls, width, label='Recall', alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('Model')\n",
    "    axes[0, 0].set_ylabel('Score')\n",
    "    axes[0, 0].set_title('Referential Ambiguity Detection Task - Precision & Recall')\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(models, rotation=45, ha='right')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 检测任务F2分数\n",
    "    axes[0, 1].bar(models, det_f2s, alpha=0.8, color='skyblue')\n",
    "    axes[0, 1].set_xlabel('Model')\n",
    "    axes[0, 1].set_ylabel('F2 Score')\n",
    "    axes[0, 1].set_title('Referential Ambiguity Detection Task - F2 Score')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 消歧任务精确率和召回率\n",
    "    axes[1, 0].bar(x - width/2, res_precisions, width, label='Precision', alpha=0.8, color='lightgreen')\n",
    "    axes[1, 0].bar(x + width/2, res_recalls, width, label='Recall', alpha=0.8, color='orange')\n",
    "    axes[1, 0].set_xlabel('Model')\n",
    "    axes[1, 0].set_ylabel('Score')\n",
    "    axes[1, 0].set_title('Anaphora Resolution Task - Precision & Recall')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(models, rotation=45, ha='right')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 消歧任务F2分数\n",
    "    axes[1, 1].bar(models, res_f2s, alpha=0.8, color='lightcoral')\n",
    "    axes[1, 1].set_xlabel('Model')\n",
    "    axes[1, 1].set_ylabel('F2 Score')\n",
    "    axes[1, 1].set_title('Anaphora Resolution Task - F2 Score')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_evaluation_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e13c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    主函数\n",
    "    \"\"\"\n",
    "    print(\"Loading test dataset...\")\n",
    "    \n",
    "    # 加载测试数据集\n",
    "    df_test = pd.read_csv(r\"data/frieden84-nlp4re-reqeval-3772a07/data/test.tsv\", sep='\\t', on_bad_lines='skip')\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # 对每个模型进行评估\n",
    "    for model in MODELS:\n",
    "        result = process_dataset(df_test, model)\n",
    "        all_results.append(result)\n",
    "        \n",
    "        # 添加延迟以避免API限制\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # 保存所有结果到JSON文件\n",
    "    with open('evaluation_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)\n",
    "    \n",
    "    print(\"\\nAll model evaluations completed, results saved to evaluation_results.json\")\n",
    "    \n",
    "    # 创建可视化图表\n",
    "    create_visualization(all_results)\n",
    "    \n",
    "    # 打印详细结果\n",
    "    print(\"\\nDetailed evaluation results:\")\n",
    "    for result in all_results:\n",
    "        model = result[\"model\"]\n",
    "        det_metrics = result[\"metrics\"][\"detection\"]\n",
    "        res_metrics = result[\"metrics\"][\"resolution\"]\n",
    "        \n",
    "        print(f\"\\nModel: {model}\")\n",
    "        print(f\"  Detection - Precision: {det_metrics['precision']:.3f}, Recall: {det_metrics['recall']:.3f}, F2: {det_metrics['f2']:.3f}\")\n",
    "        print(f\"  Resolution - Precision: {res_metrics['precision']:.3f}, Recall: {res_metrics['recall']:.3f}, F2: {res_metrics['f2']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a18d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset...\n",
      "Processing model gpt-3.5-turbo...\n",
      "  Processed 10/72 samples\n",
      "  Processed 20/72 samples\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bf2a66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
