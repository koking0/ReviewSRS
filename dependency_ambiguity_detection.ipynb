{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用户故事依赖歧义检测分析\n",
    "\n",
    "本notebook用于分析大模型在检测用户故事依赖歧义方面的性能。\n",
    "\n",
    "## 依赖歧义定义\n",
    "依赖歧义是指用户故事中的依赖关系不明确，导致对功能的实现顺序、前置条件或与其他组件/功能的关系产生理解偏差。例如：\n",
    "- 缺乏对前置条件的明确说明\n",
    "- 与其他用户故事的依赖关系不清晰\n",
    "- 系统组件间的依赖模糊\n",
    "- 数据依赖关系不明确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置信息\n",
    "CONFIG = {\n",
    "    \"base_url\": \"https://api.zhizengzeng.com/v1/\",\n",
    "    \"api_key\": \"sk-zk20f741becece1c055c848225093b2e458662329a0f1016\"\n",
    "}\n",
    "\n",
    "# 模型列表\n",
    "MODELS = [\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"claude-sonnet-4-20250514\", \n",
    "    \"gemini-2.5-flash\",\n",
    "    \"grok-3-mini\",\n",
    "    \"deepseek-chat\",\n",
    "    \"qwen3-coder-plus\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "print(\"Loading user story ambiguity dataset...\")\n",
    "df = pd.read_excel(r\"data/User Story Ambiguity Dataset_A Comprehensive Research Resource/Cornelius_2025_user_story_ambiguity_dataset.xlsx\", sheet_name='User_Stories')\n",
    "\n",
    "print(f\"数据集形状: {df.shape}\")\n",
    "print(f\"依赖歧义统计: {df['DependencyAmbiguity'].value_counts()}\")\n",
    "\n",
    "# 显示一些例子\n",
    "print(\"\\n依赖歧义示例:\")\n",
    "dependency_examples = df[df['DependencyAmbiguity'] == True][['StoryText']].head(3)\n",
    "for i, story in enumerate(dependency_examples['StoryText'], 1):\n",
    "    print(f\"{i}. {story}\")\n",
    "\n",
    "print(\"\\n无依赖歧义示例:\")\n",
    "non_dependency_examples = df[df['DependencyAmbiguity'] == False][['StoryText']].head(3)\n",
    "for i, story in enumerate(non_dependency_examples['StoryText'], 1):\n",
    "    print(f\"{i}. {story}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependency_ambiguity_prompt(story_text: str) -> str:\n",
    "    \"\"\"\n",
    "    生成用于检测用户故事依赖歧义的提示词\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "**背景**: 依赖歧义是指用户故事中的依赖关系不明确，导致对功能的实现顺序、前置条件或与其他组件/功能的关系产生理解偏差。\n",
    "\n",
    "**角色**: 你是一名专业的自然语言处理专家，专门检测软件需求规格中的依赖歧义。\n",
    "\n",
    "**任务**: 分析以下用户故事，判断其中是否存在依赖歧义。\n",
    "\n",
    "**依赖歧义的特征包括**:\n",
    "1. 缺乏对前置条件的明确说明\n",
    "2. 与其他用户故事的依赖关系不清晰\n",
    "3. 系统组件间的依赖模糊\n",
    "4. 数据依赖关系不明确\n",
    "5. 技术依赖缺乏具体描述\n",
    "6. 外部系统依赖不明确\n",
    "7. 资源依赖关系不清晰\n",
    "8. 时序依赖关系模糊\n",
    "9. 业务流程依赖不明确\n",
    "10. 环境依赖条件缺失\n",
    "\n",
    "**用户故事**: {story_text}\n",
    "\n",
    "**输出要求**:\n",
    "请按照以下JSON格式输出你的分析结果：\n",
    "{{\n",
    "    \"has_dependency_ambiguity\": true/false,\n",
    "    \"ambiguity_explanation\": \"如果存在依赖歧义，请解释歧义的具体内容和依赖关系不明确的地方；如果不存在依赖歧义，请说明为什么用户故事的依赖关系是清晰的\",\n",
    "    \"dependency_types\": \"如果存在歧义，请指出哪些类型的依赖关系不明确（如前置条件、技术依赖、数据依赖等）；如果不存在歧义，请说明已明确的依赖关系\",\n",
    "    \"missing_dependencies\": \"如果存在歧义，请列出缺失或不明确的依赖信息；如果不存在歧义，请填写'依赖关系清晰，无缺失'\",\n",
    "    \"suggested_improvement\": \"如果存在依赖歧义，请提出改进建议；如果不存在依赖歧义，请填写'无依赖歧义，无需改进'\"\n",
    "}}\n",
    "\n",
    "**注意事项**:\n",
    "- has_dependency_ambiguity的值只能是true或false\n",
    "- ambiguity_explanation应详细说明依赖歧义的原因\n",
    "- dependency_types应明确指出依赖关系的类型\n",
    "- missing_dependencies应列出缺失的依赖信息\n",
    "- suggested_improvement应提供具体的改进建议\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt: str, model: str) -> Dict:\n",
    "    \"\"\"\n",
    "    调用大模型API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = OpenAI(api_key=CONFIG[\"api_key\"], base_url=CONFIG[\"base_url\"])\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # 尝试解析JSON格式的响应\n",
    "        match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group()\n",
    "            parsed_result = json.loads(json_str)\n",
    "            return parsed_result\n",
    "        else:\n",
    "            # 如果无法解析JSON，返回默认值\n",
    "            return {\n",
    "                \"has_dependency_ambiguity\": True,\n",
    "                \"ambiguity_explanation\": \"无法解析模型响应\",\n",
    "                \"dependency_types\": \"无法解析模型响应\",\n",
    "                \"missing_dependencies\": \"无法解析模型响应\",\n",
    "                \"suggested_improvement\": \"无法解析模型响应\"\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"API调用失败 ({model}): {str(e)}\")\n",
    "        return {\n",
    "            \"has_dependency_ambiguity\": True,\n",
    "            \"ambiguity_explanation\": f\"API调用失败: {str(e)}\",\n",
    "            \"dependency_types\": f\"API调用失败: {str(e)}\",\n",
    "            \"missing_dependencies\": f\"API调用失败: {str(e)}\",\n",
    "            \"suggested_improvement\": f\"API调用失败: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dependency_detection(y_true: List[bool], y_pred: List[bool]) -> Dict:\n",
    "    \"\"\"\n",
    "    计算依赖歧义检测的评估指标\n",
    "    \"\"\"\n",
    "    tp = fp = fn = tn = 0\n",
    "    \n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        if true_label and pred_label:\n",
    "            tp += 1\n",
    "        elif not true_label and pred_label:\n",
    "            fp += 1\n",
    "        elif true_label and not pred_label:\n",
    "            fn += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn,\n",
    "        \"tn\": tn\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dependency_dataset(df_subset: pd.DataFrame, model: str) -> Dict:\n",
    "    \"\"\"\n",
    "    处理数据集子集并评估模型在依赖歧义检测上的性能\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"model\": model,\n",
    "        \"predictions\": [],\n",
    "        \"metrics\": {}\n",
    "    }\n",
    "    \n",
    "    print(f\"Processing model {model}...\")\n",
    "    \n",
    "    for idx, row in df_subset.iterrows():\n",
    "        story_text = row['StoryText']\n",
    "        true_has_ambiguity = row['DependencyAmbiguity']\n",
    "        \n",
    "        prompt = get_dependency_ambiguity_prompt(story_text)\n",
    "        prediction = call_llm(prompt, model)\n",
    "        \n",
    "        # 确保预测结果格式正确\n",
    "        if \"has_dependency_ambiguity\" not in prediction:\n",
    "            prediction[\"has_dependency_ambiguity\"] = True\n",
    "        if \"ambiguity_explanation\" not in prediction:\n",
    "            prediction[\"ambiguity_explanation\"] = \"模型未提供解释\"\n",
    "        if \"dependency_types\" not in prediction:\n",
    "            prediction[\"dependency_types\"] = \"模型未提供依赖类型信息\"\n",
    "        if \"missing_dependencies\" not in prediction:\n",
    "            prediction[\"missing_dependencies\"] = \"模型未提供缺失依赖信息\"\n",
    "        if \"suggested_improvement\" not in prediction:\n",
    "            prediction[\"suggested_improvement\"] = \"模型未提供改进建议\"\n",
    "        \n",
    "        results[\"predictions\"].append({\n",
    "            \"story_id\": row['StoryID'],\n",
    "            \"story_text\": story_text,\n",
    "            \"true_has_ambiguity\": true_has_ambiguity,\n",
    "            \"pred_has_ambiguity\": prediction[\"has_dependency_ambiguity\"],\n",
    "            \"ambiguity_explanation\": prediction[\"ambiguity_explanation\"],\n",
    "            \"dependency_types\": prediction[\"dependency_types\"],\n",
    "            \"missing_dependencies\": prediction[\"missing_dependencies\"],\n",
    "            \"suggested_improvement\": prediction[\"suggested_improvement\"]\n",
    "        })\n",
    "        \n",
    "        # 添加进度信息\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(df_subset)} samples\")\n",
    "    \n",
    "    # 计算指标\n",
    "    true_labels = [item[\"true_has_ambiguity\"] for item in results[\"predictions\"]]\n",
    "    pred_labels = [item[\"pred_has_ambiguity\"] for item in results[\"predictions\"]]\n",
    "    \n",
    "    results[\"metrics\"] = evaluate_dependency_detection(true_labels, pred_labels)\n",
    "    \n",
    "    metrics = results[\"metrics\"]\n",
    "    print(f\"  Results - Precision: {metrics['precision']:.3f}, Recall: {metrics['recall']:.3f}, F1: {metrics['f1_score']:.3f}, Accuracy: {metrics['accuracy']:.3f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dependency_visualization(results: List[Dict]):\n",
    "    \"\"\"\n",
    "    创建依赖歧义检测的可视化图表\n",
    "    \"\"\"\n",
    "    models = [r[\"model\"] for r in results]\n",
    "    \n",
    "    # 提取指标数据\n",
    "    precisions = [r[\"metrics\"][\"precision\"] for r in results]\n",
    "    recalls = [r[\"metrics\"][\"recall\"] for r in results]\n",
    "    f1_scores = [r[\"metrics\"][\"f1_score\"] for r in results]\n",
    "    accuracies = [r[\"metrics\"][\"accuracy\"] for r in results]\n",
    "    \n",
    "    # 创建图表\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    # 精确率和召回率\n",
    "    axes[0, 0].bar(x - width/2, precisions, width, label='Precision', alpha=0.8, color='wheat')\n",
    "    axes[0, 0].bar(x + width/2, recalls, width, label='Recall', alpha=0.8, color='lightpink')\n",
    "    axes[0, 0].set_xlabel('Model')\n",
    "    axes[0, 0].set_ylabel('Score')\n",
    "    axes[0, 0].set_title('依赖歧义检测 - 精确率与召回率')\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(models, rotation=45, ha='right')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[0, 0].set_ylim(0, 1)\n",
    "    \n",
    "    # F1分数\n",
    "    axes[0, 1].bar(models, f1_scores, alpha=0.8, color='peachpuff')\n",
    "    axes[0, 1].set_xlabel('Model')\n",
    "    axes[0, 1].set_ylabel('F1 Score')\n",
    "    axes[0, 1].set_title('依赖歧义检测 - F1分数')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # 准确率\n",
    "    axes[1, 0].bar(models, accuracies, alpha=0.8, color='lavender')\n",
    "    axes[1, 0].set_xlabel('Model')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    axes[1, 0].set_title('依赖歧义检测 - 准确率')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[1, 0].set_ylim(0, 1)\n",
    "    \n",
    "    # 综合性能雷达图\n",
    "    angles = np.linspace(0, 2 * np.pi, 4, endpoint=False)\n",
    "    angles = np.concatenate((angles, [angles[0]]))\n",
    "    \n",
    "    ax_radar = plt.subplot(2, 2, 4, projection='polar')\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(models)))\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        values = [precisions[i], recalls[i], f1_scores[i], accuracies[i]]\n",
    "        values = np.concatenate((values, [values[0]]))\n",
    "        ax_radar.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[i])\n",
    "        ax_radar.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "    \n",
    "    ax_radar.set_xticks(angles[:-1])\n",
    "    ax_radar.set_xticklabels(['Precision', 'Recall', 'F1', 'Accuracy'])\n",
    "    ax_radar.set_ylim(0, 1)\n",
    "    ax_radar.set_title('依赖歧义检测 - 综合性能对比')\n",
    "    ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dependency_ambiguity_detection_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    主函数：执行依赖歧义检测分析\n",
    "    \"\"\"\n",
    "    print(\"开始依赖歧义检测分析...\")\n",
    "    \n",
    "    # 准备数据集 - 从每个类别中取样\n",
    "    dependency_df = df[df['DependencyAmbiguity'] == True].sample(n=min(15, df[df['DependencyAmbiguity'] == True].shape[0]), random_state=42)\n",
    "    non_dependency_df = df[df['DependencyAmbiguity'] == False].sample(n=min(15, df[df['DependencyAmbiguity'] == False].shape[0]), random_state=42)\n",
    "    test_df = pd.concat([dependency_df, non_dependency_df]).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"测试数据集大小: {test_df.shape}\")\n",
    "    print(f\"依赖歧义样本数: {dependency_df.shape[0]}\")\n",
    "    print(f\"无依赖歧义样本数: {non_dependency_df.shape[0]}\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # 对每个模型进行评估\n",
    "    for model in MODELS:\n",
    "        result = process_dependency_dataset(test_df, model)\n",
    "        all_results.append(result)\n",
    "        \n",
    "        # 添加延迟以避免API限制\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # 保存结果\n",
    "    with open('dependency_ambiguity_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)\n",
    "    \n",
    "    print(\"\\n依赖歧义检测分析完成，结果已保存到 dependency_ambiguity_results.json\")\n",
    "    \n",
    "    # 创建可视化图表\n",
    "    create_dependency_visualization(all_results)\n",
    "    \n",
    "    # 打印详细结果\n",
    "    print(\"\\n详细评估结果:\")\n",
    "    for result in all_results:\n",
    "        model = result[\"model\"]\n",
    "        metrics = result[\"metrics\"]\n",
    "        \n",
    "        print(f\"\\n模型: {model}\")\n",
    "        print(f\"  精确率: {metrics['precision']:.3f}\")\n",
    "        print(f\"  召回率: {metrics['recall']:.3f}\")\n",
    "        print(f\"  F1分数: {metrics['f1_score']:.3f}\")\n",
    "        print(f\"  准确率: {metrics['accuracy']:.3f}\")\n",
    "        print(f\"  真正例: {metrics['tp']}, 假正例: {metrics['fp']}, 假负例: {metrics['fn']}, 真负例: {metrics['tn']}\")\n",
    "\n",
    "# 执行主函数\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}