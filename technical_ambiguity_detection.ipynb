{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用户故事技术歧义检测分析\n",
    "\n",
    "本notebook用于分析大模型在检测���户故事技术歧义方面的性能。\n",
    "\n",
    "## 技术歧义定义\n",
    "技术歧义是指用户故事中的技术实现要求、技术约束或技术细节不明确，导致开发团队对技术方案、实现方式或技术标准产生理解偏差。例如：\n",
    "- 技术架构要求不明确\n",
    "- 性能指标缺乏具体数值\n",
    "- 技术约束条件模糊\n",
    "- 集成接口细节不清晰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置信息\n",
    "CONFIG = {\n",
    "    \"base_url\": \"https://api.zhizengzeng.com/v1/\",\n",
    "    \"api_key\": \"sk-zk20f741becece1c055c848225093b2e458662329a0f1016\"\n",
    "}\n",
    "\n",
    "# 模型列表\n",
    "MODELS = [\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"claude-sonnet-4-20250514\", \n",
    "    \"gemini-2.5-flash\",\n",
    "    \"grok-3-mini\",\n",
    "    \"deepseek-chat\",\n",
    "    \"qwen3-coder-plus\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "print(\"Loading user story ambiguity dataset...\")\n",
    "df = pd.read_excel(r\"data/User Story Ambiguity Dataset_A Comprehensive Research Resource/Cornelius_2025_user_story_ambiguity_dataset.xlsx\", sheet_name='User_Stories')\n",
    "\n",
    "print(f\"数据集形状: {df.shape}\")\n",
    "print(f\"技术歧义统计: {df['TechnicalAmbiguity'].value_counts()}\")\n",
    "\n",
    "# 显示一些例子\n",
    "print(\"\\n技术歧义示例:\")\n",
    "technical_examples = df[df['TechnicalAmbiguity'] == True][['StoryText']].head(3)\n",
    "for i, story in enumerate(technical_examples['StoryText'], 1):\n",
    "    print(f\"{i}. {story}\")\n",
    "\n",
    "print(\"\\n无技术歧义示例:\")\n",
    "non_technical_examples = df[df['TechnicalAmbiguity'] == False][['StoryText']].head(3)\n",
    "for i, story in enumerate(non_technical_examples['StoryText'], 1):\n",
    "    print(f\"{i}. {story}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_technical_ambiguity_prompt(story_text: str) -> str:\n",
    "    \"\"\"\n",
    "    生成用于检测用户故事技术歧义的提示词\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "**背景**: 技术歧义是指用户故事中的技术实现要求、技术约束或技术细节不明确，导致开发团队对技术方案、实现方式或技术标准产生理解偏差。\n",
    "\n",
    "**角色**: 你是一名专业的自然语言处理专家，专门检测软件需求规格中的技术歧义。\n",
    "\n",
    "**任务**: 分析以下用户故事，判断其中是否存在技术歧义。\n",
    "\n",
    "**技术歧义的特征包括**:\n",
    "1. 技术架构要求不明确\n",
    "2. 性能指标缺乏具体数值（如响应时间、吞吐量等）\n",
    "3. 技术约束条件模糊\n",
    "4. 集成接口细节不清晰\n",
    "5. 数据格式或协议不明确\n",
    "6. 安全要求不具体\n",
    "7. 兼容性要求模糊\n",
    "8. 技术栈或框架选择不明确\n",
    "9. 部署环境要求不清晰\n",
    "10. 扩展性或可维护性要求不具体\n",
    "\n",
    "**用户故事**: {story_text}\n",
    "\n",
    "**输出要求**:\n",
    "请按照以下JSON格式输出你的分析结果：\n",
    "{{\n",
    "    \"has_technical_ambiguity\": true/false,\n",
    "    \"ambiguity_explanation\": \"如果存在技术歧义，请解释歧义的具体内容和技术要求不明确的地方；如果不存在技术歧义，请说明为什么用户故事的技术要求是清晰的\",\n",
    "    \"technical_requirements\": \"分析用户故事中的技术要求是否明确，如果存在歧义请指出需要澄清的技术细节\",\n",
    "    \"performance_criteria\": \"分析性能指标是否明确，如果存在歧义请说明性能标准的不明确之处\",\n",
    "    \"suggested_improvement\": \"如果存在技术歧义，请提出改进建议；如果不存在技术歧义，请填写'无技术歧义，无需改进'\"\n",
    "}}\n",
    "\n",
    "**注意事项**:\n",
    "- has_technical_ambiguity的值只能是true或false\n",
    "- ambiguity_explanation应详细说明技术歧义的原因\n",
    "- technical_requirements应分析技术要求的明确性\n",
    "- performance_criteria应分析性能标准的明确性\n",
    "- suggested_improvement应提供具体的改进建议\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt: str, model: str) -> Dict:\n",
    "    \"\"\"\n",
    "    调用大模型API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = OpenAI(api_key=CONFIG[\"api_key\"], base_url=CONFIG[\"base_url\"])\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # 尝试解析JSON格式的响应\n",
    "        match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group()\n",
    "            parsed_result = json.loads(json_str)\n",
    "            return parsed_result\n",
    "        else:\n",
    "            # 如果无法解析JSON，返回默认值\n",
    "            return {\n",
    "                \"has_technical_ambiguity\": True,\n",
    "                \"ambiguity_explanation\": \"无法解析模型响应\",\n",
    "                \"technical_requirements\": \"无法解析模型响应\",\n",
    "                \"performance_criteria\": \"无法解析模型响应\",\n",
    "                \"suggested_improvement\": \"无法解析模型响应\"\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"API调用失败 ({model}): {str(e)}\")\n",
    "        return {\n",
    "            \"has_technical_ambiguity\": True,\n",
    "            \"ambiguity_explanation\": f\"API调用失败: {str(e)}\",\n",
    "            \"technical_requirements\": f\"API调用失败: {str(e)}\",\n",
    "            \"performance_criteria\": f\"API调用失败: {str(e)}\",\n",
    "            \"suggested_improvement\": f\"API调用失败: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_technical_detection(y_true: List[bool], y_pred: List[bool]) -> Dict:\n",
    "    \"\"\"\n",
    "    计算技术歧义检测的评估指标\n",
    "    \"\"\"\n",
    "    tp = fp = fn = tn = 0\n",
    "    \n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        if true_label and pred_label:\n",
    "            tp += 1\n",
    "        elif not true_label and pred_label:\n",
    "            fp += 1\n",
    "        elif true_label and not pred_label:\n",
    "            fn += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn,\n",
    "        \"tn\": tn\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_technical_dataset(df_subset: pd.DataFrame, model: str) -> Dict:\n",
    "    \"\"\"\n",
    "    处理数据集子集并评估模型在技术歧义检测上的性能\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"model\": model,\n",
    "        \"predictions\": [],\n",
    "        \"metrics\": {}\n",
    "    }\n",
    "    \n",
    "    print(f\"Processing model {model}...\")\n",
    "    \n",
    "    for idx, row in df_subset.iterrows():\n",
    "        story_text = row['StoryText']\n",
    "        true_has_ambiguity = row['TechnicalAmbiguity']\n",
    "        \n",
    "        prompt = get_technical_ambiguity_prompt(story_text)\n",
    "        prediction = call_llm(prompt, model)\n",
    "        \n",
    "        # 确保预测结果格式正确\n",
    "        if \"has_technical_ambiguity\" not in prediction:\n",
    "            prediction[\"has_technical_ambiguity\"] = True\n",
    "        if \"ambiguity_explanation\" not in prediction:\n",
    "            prediction[\"ambiguity_explanation\"] = \"模型未提供解释\"\n",
    "        if \"technical_requirements\" not in prediction:\n",
    "            prediction[\"technical_requirements\"] = \"模型未提供技术要求分析\"\n",
    "        if \"performance_criteria\" not in prediction:\n",
    "            prediction[\"performance_criteria\"] = \"模型未提供性能标准分析\"\n",
    "        if \"suggested_improvement\" not in prediction:\n",
    "            prediction[\"suggested_improvement\"] = \"模型未提供改进建议\"\n",
    "        \n",
    "        results[\"predictions\"].append({\n",
    "            \"story_id\": row['StoryID'],\n",
    "            \"story_text\": story_text,\n",
    "            \"true_has_ambiguity\": true_has_ambiguity,\n",
    "            \"pred_has_ambiguity\": prediction[\"has_technical_ambiguity\"],\n",
    "            \"ambiguity_explanation\": prediction[\"ambiguity_explanation\"],\n",
    "            \"technical_requirements\": prediction[\"technical_requirements\"],\n",
    "            \"performance_criteria\": prediction[\"performance_criteria\"],\n",
    "            \"suggested_improvement\": prediction[\"suggested_improvement\"]\n",
    "        })\n",
    "        \n",
    "        # 添加进度信息\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(df_subset)} samples\")\n",
    "    \n",
    "    # 计算指标\n",
    "    true_labels = [item[\"true_has_ambiguity\"] for item in results[\"predictions\"]]\n",
    "    pred_labels = [item[\"pred_has_ambiguity\"] for item in results[\"predictions\"]]\n",
    "    \n",
    "    results[\"metrics\"] = evaluate_technical_detection(true_labels, pred_labels)\n",
    "    \n",
    "    metrics = results[\"metrics\"]\n",
    "    print(f\"  Results - Precision: {metrics['precision']:.3f}, Recall: {metrics['recall']:.3f}, F1: {metrics['f1_score']:.3f}, Accuracy: {metrics['accuracy']:.3f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_technical_visualization(results: List[Dict]):\n",
    "    \"\"\"\n",
    "    创建技术歧义检测的可视化图表\n",
    "    \"\"\"\n",
    "    models = [r[\"model\"] for r in results]\n",
    "    \n",
    "    # 提取指标数据\n",
    "    precisions = [r[\"metrics\"][\"precision\"] for r in results]\n",
    "    recalls = [r[\"metrics\"][\"recall\"] for r in results]\n",
    "    f1_scores = [r[\"metrics\"][\"f1_score\"] for r in results]\n",
    "    accuracies = [r[\"metrics\"][\"accuracy\"] for r in results]\n",
    "    \n",
    "    # 创建图表\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    # 精确率和召回率\n",
    "    axes[0, 0].bar(x - width/2, precisions, width, label='Precision', alpha=0.8, color='lightcyan')\n",
    "    axes[0, 0].bar(x + width/2, recalls, width, label='Recall', alpha=0.8, color='lightgray')\n",
    "    axes[0, 0].set_xlabel('Model')\n",
    "    axes[0, 0].set_ylabel('Score')\n",
    "    axes[0, 0].set_title('技术歧义检测 - 精确率与召回率')\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(models, rotation=45, ha='right')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[0, 0].set_ylim(0, 1)\n",
    "    \n",
    "    # F1分数\n",
    "    axes[0, 1].bar(models, f1_scores, alpha=0.8, color='beige')\n",
    "    axes[0, 1].set_xlabel('Model')\n",
    "    axes[0, 1].set_ylabel('F1 Score')\n",
    "    axes[0, 1].set_title('技术歧义检测 - F1分数')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # 准确率\n",
    "    axes[1, 0].bar(models, accuracies, alpha=0.8, color='aliceblue')\n",
    "    axes[1, 0].set_xlabel('Model')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    axes[1, 0].set_title('技术歧义检测 - 准确率')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[1, 0].set_ylim(0, 1)\n",
    "    \n",
    "    # 综合性能雷达图\n",
    "    angles = np.linspace(0, 2 * np.pi, 4, endpoint=False)\n",
    "    angles = np.concatenate((angles, [angles[0]]))\n",
    "    \n",
    "    ax_radar = plt.subplot(2, 2, 4, projection='polar')\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(models)))\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        values = [precisions[i], recalls[i], f1_scores[i], accuracies[i]]\n",
    "        values = np.concatenate((values, [values[0]]))\n",
    "        ax_radar.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[i])\n",
    "        ax_radar.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "    \n",
    "    ax_radar.set_xticks(angles[:-1])\n",
    "    ax_radar.set_xticklabels(['Precision', 'Recall', 'F1', 'Accuracy'])\n",
    "    ax_radar.set_ylim(0, 1)\n",
    "    ax_radar.set_title('技术歧义检测 - 综合性能对比')\n",
    "    ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('technical_ambiguity_detection_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    主函数：执行技术歧义检测分析\n",
    "    \"\"\"\n",
    "    print(\"开始技术歧义检测分析...\")\n",
    "    \n",
    "    # 准备数据集 - 从每个类别中取样\n",
    "    technical_df = df[df['TechnicalAmbiguity'] == True].sample(n=min(5, df[df['TechnicalAmbiguity'] == True].shape[0]), random_state=42)\n",
    "    non_technical_df = df[df['TechnicalAmbiguity'] == False].sample(n=min(15, df[df['TechnicalAmbiguity'] == False].shape[0]), random_state=42)\n",
    "    test_df = pd.concat([technical_df, non_technical_df]).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"测试数据集大小: {test_df.shape}\")\n",
    "    print(f\"技术歧义样本数: {technical_df.shape[0]}\")\n",
    "    print(f\"无技术歧义样本数: {non_technical_df.shape[0]}\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # 对每个模型进行评估\n",
    "    for model in MODELS:\n",
    "        result = process_technical_dataset(test_df, model)\n",
    "        all_results.append(result)\n",
    "        \n",
    "        # 添加延迟以避免API限制\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # 保存结果\n",
    "    with open('technical_ambiguity_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)\n",
    "    \n",
    "    print(\"\\n技术歧义检测分析完成，结果已保存到 technical_ambiguity_results.json\")\n",
    "    \n",
    "    # 创建可视化图表\n",
    "    create_technical_visualization(all_results)\n",
    "    \n",
    "    # 打印详细结果\n",
    "    print(\"\\n详细评估结果:\")\n",
    "    for result in all_results:\n",
    "        model = result[\"model\"]\n",
    "        metrics = result[\"metrics\"]\n",
    "        \n",
    "        print(f\"\\n模型: {model}\")\n",
    "        print(f\"  精确率: {metrics['precision']:.3f}\")\n",
    "        print(f\"  召回率: {metrics['recall']:.3f}\")\n",
    "        print(f\"  F1分数: {metrics['f1_score']:.3f}\")\n",
    "        print(f\"  准确率: {metrics['accuracy']:.3f}\")\n",
    "        print(f\"  真正例: {metrics['tp']}, 假正例: {metrics['fp']}, 假负例: {metrics['fn']}, 真负例: {metrics['tn']}\")\n",
    "\n",
    "# 执行主函数\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}