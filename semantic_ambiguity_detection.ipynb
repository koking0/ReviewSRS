{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用户故事语义歧义检测分析\n",
    "\n",
    "本notebook用于分析大模型在检测用户故事语义歧义方面的性能。\n",
    "\n",
    "## 语义歧义定义\n",
    "语义歧义是指用户故事中的词汇、短语或句子可以有多种解释，导致理解上的混淆。例如：\n",
    "- 使用模糊或不精确的术语\n",
    "- 存在多种合理的解释\n",
    "- 缺乏足够的上下文信息来确定唯一含义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置信息\n",
    "CONFIG = {\n",
    "    \"base_url\": \"https://api.zhizengzeng.com/v1/\",\n",
    "    \"api_key\": \"sk-zk20f741becece1c055c848225093b2e458662329a0f1016\"\n",
    "}\n",
    "\n",
    "# 模型列表\n",
    "MODELS = [\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"claude-sonnet-4-20250514\", \n",
    "    \"gemini-2.5-flash\",\n",
    "    \"grok-3-mini\",\n",
    "    \"deepseek-chat\",\n",
    "    \"qwen3-coder-plus\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "print(\"Loading user story ambiguity dataset...\")\n",
    "df = pd.read_excel(r\"data/User Story Ambiguity Dataset_A Comprehensive Research Resource/Cornelius_2025_user_story_ambiguity_dataset.xlsx\", sheet_name='User_Stories')\n",
    "\n",
    "print(f\"数据集形状: {df.shape}\")\n",
    "print(f\"语义歧义统计: {df['SemanticAmbiguity'].value_counts()}\")\n",
    "\n",
    "# 显示一些例子\n",
    "print(\"\\n语义歧义示例:\")\n",
    "semantic_examples = df[df['SemanticAmbiguity'] == True][['StoryText']].head(3)\n",
    "for i, story in enumerate(semantic_examples['StoryText'], 1):\n",
    "    print(f\"{i}. {story}\")\n",
    "\n",
    "print(\"\\n无语义歧义示例:\")\n",
    "non_semantic_examples = df[df['SemanticAmbiguity'] == False][['StoryText']].head(3)\n",
    "for i, story in enumerate(non_semantic_examples['StoryText'], 1):\n",
    "    print(f\"{i}. {story}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_ambiguity_prompt(story_text: str) -> str:\n",
    "    \"\"\"\n",
    "    生成用于检测用户故事语义歧义的提示词\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "**背景**: 语义歧义是指用户故事中的词汇、短语或句子可以有多种解释，导致���解上的混淆。\n",
    "\n",
    "**角色**: 你是一名专业的自然语言处理专家，专门检测软件需求规格中的语义歧义。\n",
    "\n",
    "**任务**: 分析以下用户故事，判断其中是否存在语义歧义。\n",
    "\n",
    "**语义歧义的特征包括**:\n",
    "1. 使用模糊或不精确的术语\n",
    "2. 存在多种合理的解释\n",
    "3. 缺乏足够的上下文信息来确定唯一含义\n",
    "4. 词汇可以有多种理解方式\n",
    "5. 句子结构不清晰导致多种解释\n",
    "\n",
    "**用户故事**: {story_text}\n",
    "\n",
    "**输出要求**:\n",
    "请按照以下JSON格式输出你的分析结果：\n",
    "{{\n",
    "    \"has_semantic_ambiguity\": true/false,\n",
    "    \"ambiguity_explanation\": \"如果存在歧义，请解释歧义的具体内容和可能的多种解释；如果不存在歧义，请说明为什么用户故事是清晰的\",\n",
    "    \"suggested_improvement\": \"如果存在歧义，请提出改进建议；如果不存在歧义，请填写'无歧义，无需改进'\"\n",
    "}}\n",
    "\n",
    "**注意事项**:\n",
    "- has_semantic_ambiguity的值只能是true或false\n",
    "- ambiguity_explanation应详细说明歧义原因\n",
    "- suggested_improvement应提供具体的改进建议\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt: str, model: str) -> Dict:\n",
    "    \"\"\"\n",
    "    调用大模型API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = OpenAI(api_key=CONFIG[\"api_key\"], base_url=CONFIG[\"base_url\"])\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # 尝试解析JSON格式的响应\n",
    "        match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group()\n",
    "            parsed_result = json.loads(json_str)\n",
    "            return parsed_result\n",
    "        else:\n",
    "            # 如果无法解析JSON，返回默认值\n",
    "            return {\n",
    "                \"has_semantic_ambiguity\": True,\n",
    "                \"ambiguity_explanation\": \"无法解析模型响应\",\n",
    "                \"suggested_improvement\": \"无法解析模型响应\"\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"API调用失败 ({model}): {str(e)}\")\n",
    "        return {\n",
    "            \"has_semantic_ambiguity\": True,\n",
    "            \"ambiguity_explanation\": f\"API调用失败: {str(e)}\",\n",
    "            \"suggested_improvement\": f\"API调用失败: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_semantic_detection(y_true: List[bool], y_pred: List[bool]) -> Dict:\n",
    "    \"\"\"\n",
    "    计算语义歧义检测的评估指标\n",
    "    \"\"\"\n",
    "    tp = fp = fn = tn = 0\n",
    "    \n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        if true_label and pred_label:\n",
    "            tp += 1\n",
    "        elif not true_label and pred_label:\n",
    "            fp += 1\n",
    "        elif true_label and not pred_label:\n",
    "            fn += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn,\n",
    "        \"tn\": tn\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_semantic_dataset(df_subset: pd.DataFrame, model: str) -> Dict:\n",
    "    \"\"\"\n",
    "    处理数据集子集并评估模型在语义歧义检测上的性能\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"model\": model,\n",
    "        \"predictions\": [],\n",
    "        \"metrics\": {}\n",
    "    }\n",
    "    \n",
    "    print(f\"Processing model {model}...\")\n",
    "    \n",
    "    for idx, row in df_subset.iterrows():\n",
    "        story_text = row['StoryText']\n",
    "        true_has_ambiguity = row['SemanticAmbiguity']\n",
    "        \n",
    "        prompt = get_semantic_ambiguity_prompt(story_text)\n",
    "        prediction = call_llm(prompt, model)\n",
    "        \n",
    "        # 确保预测结果格式正确\n",
    "        if \"has_semantic_ambiguity\" not in prediction:\n",
    "            prediction[\"has_semantic_ambiguity\"] = True\n",
    "        if \"ambiguity_explanation\" not in prediction:\n",
    "            prediction[\"ambiguity_explanation\"] = \"模型未提供解释\"\n",
    "        if \"suggested_improvement\" not in prediction:\n",
    "            prediction[\"suggested_improvement\"] = \"模型未提供改进建议\"\n",
    "        \n",
    "        results[\"predictions\"].append({\n",
    "            \"story_id\": row['StoryID'],\n",
    "            \"story_text\": story_text,\n",
    "            \"true_has_ambiguity\": true_has_ambiguity,\n",
    "            \"pred_has_ambiguity\": prediction[\"has_semantic_ambiguity\"],\n",
    "            \"ambiguity_explanation\": prediction[\"ambiguity_explanation\"],\n",
    "            \"suggested_improvement\": prediction[\"suggested_improvement\"]\n",
    "        })\n",
    "        \n",
    "        # 添加进度信息\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(df_subset)} samples\")\n",
    "    \n",
    "    # 计算指标\n",
    "    true_labels = [item[\"true_has_ambiguity\"] for item in results[\"predictions\"]]\n",
    "    pred_labels = [item[\"pred_has_ambiguity\"] for item in results[\"predictions\"]]\n",
    "    \n",
    "    results[\"metrics\"] = evaluate_semantic_detection(true_labels, pred_labels)\n",
    "    \n",
    "    metrics = results[\"metrics\"]\n",
    "    print(f\"  Results - Precision: {metrics['precision']:.3f}, Recall: {metrics['recall']:.3f}, F1: {metrics['f1_score']:.3f}, Accuracy: {metrics['accuracy']:.3f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_semantic_visualization(results: List[Dict]):\n",
    "    \"\"\"\n",
    "    创建语义歧义检测的可视化图表\n",
    "    \"\"\"\n",
    "    models = [r[\"model\"] for r in results]\n",
    "    \n",
    "    # 提取指标数据\n",
    "    precisions = [r[\"metrics\"][\"precision\"] for r in results]\n",
    "    recalls = [r[\"metrics\"][\"recall\"] for r in results]\n",
    "    f1_scores = [r[\"metrics\"][\"f1_score\"] for r in results]\n",
    "    accuracies = [r[\"metrics\"][\"accuracy\"] for r in results]\n",
    "    \n",
    "    # 创建图表\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    # 精确率和召回率\n",
    "    axes[0, 0].bar(x - width/2, precisions, width, label='Precision', alpha=0.8, color='skyblue')\n",
    "    axes[0, 0].bar(x + width/2, recalls, width, label='Recall', alpha=0.8, color='lightgreen')\n",
    "    axes[0, 0].set_xlabel('Model')\n",
    "    axes[0, 0].set_ylabel('Score')\n",
    "    axes[0, 0].set_title('语义歧义检测 - 精确率与召回率')\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(models, rotation=45, ha='right')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[0, 0].set_ylim(0, 1)\n",
    "    \n",
    "    # F1分数\n",
    "    axes[0, 1].bar(models, f1_scores, alpha=0.8, color='orange')\n",
    "    axes[0, 1].set_xlabel('Model')\n",
    "    axes[0, 1].set_ylabel('F1 Score')\n",
    "    axes[0, 1].set_title('语义歧义检测 - F1分数')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # 准确率\n",
    "    axes[1, 0].bar(models, accuracies, alpha=0.8, color='purple')\n",
    "    axes[1, 0].set_xlabel('Model')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    axes[1, 0].set_title('语义歧义检测 - 准确率')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[1, 0].set_ylim(0, 1)\n",
    "    \n",
    "    # 综合性能雷达图\n",
    "    angles = np.linspace(0, 2 * np.pi, 4, endpoint=False)\n",
    "    angles = np.concatenate((angles, [angles[0]]))\n",
    "    \n",
    "    ax_radar = plt.subplot(2, 2, 4, projection='polar')\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(models)))\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        values = [precisions[i], recalls[i], f1_scores[i], accuracies[i]]\n",
    "        values = np.concatenate((values, [values[0]]))\n",
    "        ax_radar.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[i])\n",
    "        ax_radar.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "    \n",
    "    ax_radar.set_xticks(angles[:-1])\n",
    "    ax_radar.set_xticklabels(['Precision', 'Recall', 'F1', 'Accuracy'])\n",
    "    ax_radar.set_ylim(0, 1)\n",
    "    ax_radar.set_title('语义歧义检测 - 综合性能对比')\n",
    "    ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('semantic_ambiguity_detection_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    主函数：执行语义歧义检测分析\n",
    "    \"\"\"\n",
    "    print(\"开始语义歧义检测分析...\")\n",
    "    \n",
    "    # 准备数据集 - 从每个类别中取样\n",
    "    semantic_df = df[df['SemanticAmbiguity'] == True].sample(n=50, random_state=42)\n",
    "    non_semantic_df = df[df['SemanticAmbiguity'] == False].sample(n=50, random_state=42)\n",
    "    test_df = pd.concat([semantic_df, non_semantic_df]).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"测试数据集大小: {test_df.shape}\")\n",
    "    print(f\"语义歧义样本数: {semantic_df.shape[0]}\")\n",
    "    print(f\"无语义歧义样本数: {non_semantic_df.shape[0]}\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # 对每个模型进行评估\n",
    "    for model in MODELS:\n",
    "        result = process_semantic_dataset(test_df, model)\n",
    "        all_results.append(result)\n",
    "        \n",
    "        # 添加延迟以避免API限制\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # 保存结果\n",
    "    with open('semantic_ambiguity_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)\n",
    "    \n",
    "    print(\"\\n语义歧义检测分析完成，结果已保存到 semantic_ambiguity_results.json\")\n",
    "    \n",
    "    # 创建可视化图表\n",
    "    create_semantic_visualization(all_results)\n",
    "    \n",
    "    # 打印详细结果\n",
    "    print(\"\\n详细评估结果:\")\n",
    "    for result in all_results:\n",
    "        model = result[\"model\"]\n",
    "        metrics = result[\"metrics\"]\n",
    "        \n",
    "        print(f\"\\n模型: {model}\")\n",
    "        print(f\"  精确率: {metrics['precision']:.3f}\")\n",
    "        print(f\"  召回率: {metrics['recall']:.3f}\")\n",
    "        print(f\"  F1分数: {metrics['f1_score']:.3f}\")\n",
    "        print(f\"  准确率: {metrics['accuracy']:.3f}\")\n",
    "        print(f\"  真正例: {metrics['tp']}, 假正例: {metrics['fp']}, 假负例: {metrics['fn']}, 真负例: {metrics['tn']}\")\n",
    "\n",
    "# 执行主函数\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}