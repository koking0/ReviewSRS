{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用户故事��义检测综合分析\n",
    "\n",
    "本notebook对7种不同类型的用户故事歧义进行综合分析和比较。\n",
    "\n",
    "## 歧义类型概览\n",
    "1. **语义歧义 (Semantic Ambiguity)** - 词汇或短语的多种含义解释\n",
    "2. **范围歧义 (Scope Ambiguity)** - 功能边界和适用条件不明确\n",
    "3. **角色歧义 (Actor Ambiguity)** - 参与者或系统角色不明确\n",
    "4. **验收标准歧义 (Acceptance Ambiguity)** - 完成标准无法客观验证\n",
    "5. **依赖歧义 (Dependency Ambiguity)** - 外部依赖或系统集成不明确\n",
    "6. **优先级歧义 (Priority Ambiguity)** - 功能重要性或实现顺序不明确\n",
    "7. **技术歧义 (Technical Ambiguity)** - 技术实现方案或架构不明确\n",
    "\n",
    "## 分析目标\n",
    "- 比较不同模型在各种歧义类型上的表现\n",
    "- 分析各歧义类型的检测难度差异\n",
    "- 发现模型的优势和不足\n",
    "- 为实际应用提供指导建议"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置显示选项\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# 设置图表样式\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义歧义类型和对应的文件名\n",
    "AMBIGUITY_TYPES = [\n",
    "    \"semantic\",\n",
    "    \"scope\", \n",
    "    \"actor\",\n",
    "    \"acceptance\",\n",
    "    \"dependency\",\n",
    "    \"priority\",\n",
    "    \"technical\"\n",
    "]\n",
    "\n",
    "RESULT_FILES = {\n",
    "    \"semantic\": \"semantic_ambiguity_evaluation_results.json\",\n",
    "    \"scope\": \"scope_ambiguity_evaluation_results.json\",\n",
    "    \"actor\": \"actor_ambiguity_evaluation_results.json\",\n",
    "    \"acceptance\": \"acceptance_ambiguity_evaluation_results.json\",\n",
    "    \"dependency\": \"dependency_ambiguity_evaluation_results.json\",\n",
    "    \"priority\": \"priority_ambiguity_evaluation_results.json\",\n",
    "    \"technical\": \"technical_ambiguity_evaluation_results.json\"\n",
    "}\n",
    "\n",
    "MODEL_NAMES = [\"gpt-3.5-turbo\", \"gemini-2.5-flash\", \"deepseek-chat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载所有评估结果\n",
    "def load_all_results():\n",
    "    \"\"\"\n",
    "    加载所有歧义类型的评估结果\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    \n",
    "    for amb_type, filename in RESULT_FILES.items():\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                results = json.load(f)\n",
    "                all_results[amb_type] = results\n",
    "                print(f\"✓ 加载 {amb_type} 歧义结果: {len(results)} 个模型\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"✗ 未找到 {amb_type} 歧义结果文件: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ 加载 {amb_type} 歧义结果时出错: {e}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "all_results = load_all_results()\n",
    "print(f\"\\n成功加载 {len(all_results)} 种歧义类型的评估结果\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建结果汇总表\n",
    "def create_summary_table(all_results):\n",
    "    \"\"\"\n",
    "    创建所有结果的汇总表\n",
    "    \"\"\"\n",
    "    summary_data = []\n",
    "    \n",
    "    for amb_type, results in all_results.items():\n",
    "        for result in results:\n",
    "            model = result['model']\n",
    "            metrics = result['metrics']\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Ambiguity Type': amb_type.title(),\n",
    "                'Model': model,\n",
    "                'Precision': metrics['precision'],\n",
    "                'Recall': metrics['recall'],\n",
    "                'F1 Score': metrics['f1']\n",
    "            })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    return summary_df\n",
    "\n",
    "summary_df = create_summary_table(all_results)\n",
    "print(\"\\n=== 评估结果汇总表 ===\")\n",
    "print(summary_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建综合可视化\n",
    "def create_comprehensive_visualization(summary_df):\n",
    "    \"\"\"\n",
    "    创建综合的可视化图表\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    fig.suptitle('User Story Ambiguity Detection: Comprehensive Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. 总体性能热力图\n",
    "    ax1 = plt.subplot(3, 3, 1)\n",
    "    pivot_f1 = summary_df.pivot(index='Model', columns='Ambiguity Type', values='F1 Score')\n",
    "    sns.heatmap(pivot_f1, annot=True, fmt='.3f', cmap='RdYlBu_r', ax=ax1,\n",
    "                cbar_kws={'label': 'F1 Score'})\n",
    "    ax1.set_title('F1 Score Heatmap\\n(Models vs Ambiguity Types)')\n",
    "    ax1.set_xlabel('')\n",
    "    ax1.set_ylabel('')\n",
    "    \n",
    "    # 2. 各模型平均性能对比\n",
    "    ax2 = plt.subplot(3, 3, 2)\n",
    "    model_avg = summary_df.groupby('Model')[['Precision', 'Recall', 'F1 Score']].mean()\n",
    "    model_avg.plot(kind='bar', ax=ax2, alpha=0.8)\n",
    "    ax2.set_title('Average Performance by Model')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 3. 各歧义类型平均难度对比\n",
    "    ax3 = plt.subplot(3, 3, 3)\n",
    "    type_avg = summary_df.groupby('Ambiguity Type')[['Precision', 'Recall', 'F1 Score']].mean()\n",
    "    type_avg.plot(kind='bar', ax=ax3, alpha=0.8)\n",
    "    ax3.set_title('Average Detection Difficulty by Ambiguity Type')\n",
    "    ax3.set_ylabel('Score')\n",
    "    ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax3.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. 精确率对比（按歧义类型分组）\n",
    "    ax4 = plt.subplot(3, 3, 4)\n",
    "    pivot_precision = summary_df.pivot(index='Model', columns='Ambiguity Type', values='Precision')\n",
    "    pivot_precision.plot(kind='bar', ax=ax4, alpha=0.7)\n",
    "    ax4.set_title('Precision Comparison\\n(by Ambiguity Type)')\n",
    "    ax4.set_ylabel('Precision')\n",
    "    ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax4.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 5. 召回率对比（按歧义类型分组）\n",
    "    ax5 = plt.subplot(3, 3, 5)\n",
    "    pivot_recall = summary_df.pivot(index='Model', columns='Ambiguity Type', values='Recall')\n",
    "    pivot_recall.plot(kind='bar', ax=ax5, alpha=0.7)\n",
    "    ax5.set_title('Recall Comparison\\n(by Ambiguity Type)')\n",
    "    ax5.set_ylabel('Recall')\n",
    "    ax5.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax5.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 6. F1分数对比（按歧义类型分组）\n",
    "    ax6 = plt.subplot(3, 3, 6)\n",
    "    pivot_f1.plot(kind='bar', ax=ax6, alpha=0.7)\n",
    "    ax6.set_title('F1 Score Comparison\\n(by Ambiguity Type)')\n",
    "    ax6.set_ylabel('F1 Score')\n",
    "    ax6.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax6.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 7. 模型排名（按F1分数）\n",
    "    ax7 = plt.subplot(3, 3, 7)\n",
    "    best_performance = summary_df.loc[summary_df.groupby('Ambiguity Type')['F1 Score'].idxmax()]\n",
    "    best_model_counts = best_performance['Model'].value_counts()\n",
    "    colors = ['gold', 'silver', '#CD7F32']  # 金银铜色\n",
    "    best_model_counts.plot(kind='bar', ax=ax7, color=colors[:len(best_model_counts)])\n",
    "    ax7.set_title('Best Model Count\\n(Number of ambiguity types where model performs best)')\n",
    "    ax7.set_ylabel('Count')\n",
    "    ax7.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 8. 性能分布箱线图\n",
    "    ax8 = plt.subplot(3, 3, 8)\n",
    "    summary_df.boxplot(column='F1 Score', by='Model', ax=ax8)\n",
    "    ax8.set_title('F1 Score Distribution\\nby Model')\n",
    "    ax8.set_xlabel('')\n",
    "    ax8.set_ylabel('F1 Score')\n",
    "    \n",
    "    # 9. 性能分布箱线图（按歧义类型）\n",
    "    ax9 = plt.subplot(3, 3, 9)\n",
    "    summary_df.boxplot(column='F1 Score', by='Ambiguity Type', ax=ax9)\n",
    "    ax9.set_title('F1 Score Distribution\\nby Ambiguity Type')\n",
    "    ax9.set_xlabel('')\n",
    "    ax9.set_ylabel('F1 Score')\n",
    "    ax9.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comprehensive_ambiguity_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "create_comprehensive_visualization(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析最佳和最差表现\n",
    "def analyze_extremes(summary_df):\n",
    "    \"\"\"\n",
    "    分析最佳和最差表现\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 最佳和最差表现分析 ===\")\n",
    "    \n",
    "    # 最佳F1分数\n",
    "    best_overall = summary_df.loc[summary_df['F1 Score'].idxmax()]\n",
    "    print(f\"\\n最佳整体表现:\")\n",
    "    print(f\"  模型: {best_overall['Model']}\")\n",
    "    print(f\"  歧义类型: {best_overall['Ambiguity Type']}\")\n",
    "    print(f\"  F1分数: {best_overall['F1 Score']:.3f}\")\n",
    "    \n",
    "    # 最差F1分数\n",
    "    worst_overall = summary_df.loc[summary_df['F1 Score'].idxmin()]\n",
    "    print(f\"\\n最差整体表现:\")\n",
    "    print(f\"  模型: {worst_overall['Model']}\")\n",
    "    print(f\"  歧义类型: {worst_overall['Ambiguity Type']}\")\n",
    "    print(f\"  F1分数: {worst_overall['F1 Score']:.3f}\")\n",
    "    \n",
    "    # 按歧义类型分析最佳模型\n",
    "    print(f\"\\n各歧义类型的最佳模型:\")\n",
    "    best_by_type = summary_df.loc[summary_df.groupby('Ambiguity Type')['F1 Score'].idxmax()]\n",
    "    for _, row in best_by_type.iterrows():\n",
    "        print(f\"  {row['Ambiguity Type']}: {row['Model']} (F1: {row['F1 Score']:.3f})\")\n",
    "    \n",
    "    # 按模型分析最佳歧义类型\n",
    "    print(f\"\\n各模型的最佳歧义类型:\")\n",
    "    best_by_model = summary_df.loc[summary_df.groupby('Model')['F1 Score'].idxmax()]\n",
    "    for _, row in best_by_model.iterrows():\n",
    "        print(f\"  {row['Model']}: {row['Ambiguity Type']} (F1: {row['F1 Score']:.3f})\")\n",
    "\n",
    "analyze_extremes(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检测难度分析\n",
    "def analyze_difficulty(summary_df):\n",
    "    \"\"\"\n",
    "    分析不同歧义类型的检测难度\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 歧义类型检测难度分析 ===\")\n",
    "    \n",
    "    # 计算每个歧义类型的平均性能\n",
    "    type_stats = summary_df.groupby('Ambiguity Type').agg({\n",
    "        'Precision': ['mean', 'std'],\n",
    "        'Recall': ['mean', 'std'],\n",
    "        'F1 Score': ['mean', 'std']\n",
    "    }).round(3)\n",
    "    \n",
    "    print(\"\\n各歧义类型的平均性能统计:\")\n",
    "    print(type_stats)\n",
    "    \n",
    "    # 按平均F1分数排序\n",
    "    avg_f1 = summary_df.groupby('Ambiguity Type')['F1 Score'].mean().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\n按检测难度排序（从易到难，按平均F1分数）:\")\n",
    "    for i, (amb_type, f1) in enumerate(avg_f1.items(), 1):\n",
    "        difficulty = \"容易\" if f1 > 0.7 else \"中等\" if f1 > 0.5 else \"困难\"\n",
    "        print(f\"  {i}. {amb_type}: F1={f1:.3f} ({difficulty})\")\n",
    "    \n",
    "    # 计算变异系数（衡量一致性）\n",
    "    cv_by_type = summary_df.groupby('Ambiguity Type')['F1 Score'].std() / summary_df.groupby('Ambiguity Type')['F1 Score'].mean()\n",
    "    \n",
    "    print(\"\\n模型间一致性（变异系数，越小越一致）:\")\n",
    "    for amb_type, cv in cv_by_type.sort_values().items():\n",
    "        consistency = \"高度一致\" if cv < 0.1 else \"比较一致\" if cv < 0.2 else \"不一致\"\n",
    "        print(f\"  {amb_type}: CV={cv:.3f} ({consistency})\")\n",
    "\n",
    "analyze_difficulty(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型优势分析\n",
    "def analyze_model_strengths(summary_df):\n",
    "    \"\"\"\n",
    "    分析每个模型的优势\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 模型优势分析 ===\")\n",
    "    \n",
    "    for model in MODEL_NAMES:\n",
    "        if model in summary_df['Model'].values:\n",
    "            model_data = summary_df[summary_df['Model'] == model]\n",
    "            \n",
    "            # 计算平均性能\n",
    "            avg_precision = model_data['Precision'].mean()\n",
    "            avg_recall = model_data['Recall'].mean()\n",
    "            avg_f1 = model_data['F1 Score'].mean()\n",
    "            \n",
    "            # 找出最佳表现的歧义类型\n",
    "            best_type = model_data.loc[model_data['F1 Score'].idxmax()]\n",
    "            worst_type = model_data.loc[model_data['F1 Score'].idxmin()]\n",
    "            \n",
    "            print(f\"\\n{model} 模型分析:\")\n",
    "            print(f\"  平均精确率: {avg_precision:.3f}\")\n",
    "            print(f\"  平均召回率: {avg_recall:.3f}\")\n",
    "            print(f\"  平均F1分数: {avg_f1:.3f}\")\n",
    "            print(f\"  最佳表现: {best_type['Ambiguity Type']} (F1: {best_type['F1 Score']:.3f})\")\n",
    "            print(f\"  最差表现: {worst_type['Ambiguity Type']} (F1: {worst_type['F1 Score']:.3f})\")\n",
    "            \n",
    "            # 判断模型特点\n",
    "            if avg_precision > avg_recall:\n",
    "                style = \"精确型（更注重准确性）\"\n",
    "            elif avg_recall > avg_precision:\n",
    "                style = \"召回型（更注重覆盖性）\"\n",
    "            else:\n",
    "                style = \"平衡型\"\n",
    "            \n",
    "            print(f\"  模型特点: {style}\")\n",
    "\n",
    "analyze_model_strengths(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存综合分析结果\n",
    "def save_comprehensive_results(summary_df):\n",
    "    \"\"\"\n",
    "    保存综合分析结果\n",
    "    \"\"\"\n",
    "    # 保存汇总表\n",
    "    summary_df.to_csv('comprehensive_ambiguity_summary.csv', index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # 创建详细报告\n",
    "    report = {\n",
    "        \"analysis_date\": pd.Timestamp.now().isoformat(),\n",
    "        \"total_ambiguity_types\": len(AMBIGUITY_TYPES),\n",
    "        \"total_models\": len(MODEL_NAMES),\n",
    "        \"summary_statistics\": {\n",
    "            \"best_overall_performance\": summary_df.loc[summary_df['F1 Score'].idxmax()].to_dict(),\n",
    "            \"worst_overall_performance\": summary_df.loc[summary_df['F1 Score'].idxmin()].to_dict(),\n",
    "            \"average_f1_by_model\": summary_df.groupby('Model')['F1 Score'].mean().to_dict(),\n",
    "            \"average_f1_by_type\": summary_df.groupby('Ambiguity Type')['F1 Score'].mean().to_dict()\n",
    "        },\n",
    "        \"detailed_results\": summary_df.to_dict('records')\n",
    "    }\n",
    "    \n",
    "    with open('comprehensive_ambiguity_analysis_report.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, ensure_ascii=False, indent=2, default=str)\n",
    "    \n",
    "    print(\"\\n=== 保存文件 ===\")\n",
    "    print(\"✓ comprehensive_ambiguity_summary.csv - 汇总数据表\")\n",
    "    print(\"✓ comprehensive_ambiguity_analysis_report.json - 详细分析报告\")\n",
    "    print(\"✓ comprehensive_ambiguity_analysis.png - 可视化图表\")\n",
    "\n",
    "save_comprehensive_results(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实用建议\n",
    "def provide_recommendations(summary_df):\n",
    "    \"\"\"\n",
    "    基于分析结果提供实用建议\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 实用建议 ===\")\n",
    "    \n",
    "    # 找出整体最佳模型\n",
    "    best_model = summary_df.groupby('Model')['F1 Score'].mean().idxmax()\n",
    "    print(f\"\\n1. 模型选择建议:\")\n",
    "    print(f\"   推荐使用 {best_model} 作为主要检测模型，因为它在所有歧义类型上表现最稳定。\")\n",
    "    \n",
    "    # 找出最容易和最难的歧义类型\n",
    "    avg_f1 = summary_df.groupby('Ambiguity Type')['F1 Score'].mean()\n",
    "    easiest_type = avg_f1.idxmax()\n",
    "    hardest_type = avg_f1.idxmin()\n",
    "    \n",
    "    print(f\"\\n2. 歧义检测难度:\")\n",
    "    print(f\"   最容易检测: {easiest_type} (平均F1: {avg_f1.max():.3f})\")\n",
    "    print(f\"   最难检测: {hardest_type} (平均F1: {avg_f1.min():.3f})\")\n",
    "    print(f\"   建议: 对于{hardest_type}，可能需要更专门的提示词或人工复核。\")\n",
    "    \n",
    "    # 精确率vs召回率分析\n",
    "    model_styles = summary_df.groupby('Model').apply(\n",
    "        lambda x: 'Precision-focused' if x['Precision'].mean() > x['Recall'].mean() else 'Recall-focused'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n3. 应用场景建议:\")\n",
    "    for model, style in model_styles.items():\n",
    "        if 'Precision' in style:\n",
    "            scenario = \"适合需要高准确性的场景，避免误报\"\n",
    "        else:\n",
    "            scenario = \"适合需要全面检测的场景，避免漏报\"\n",
    "        print(f\"   {model}: {scenario}\")\n",
    "    \n",
    "    print(f\"\\n4. 改进建议:\")\n",
    "    print(f\"   - 针对困难歧义类型开发专门的提示词模板\")\n",
    "    print(f\"   - 考虑使用模型集成方法提高整体性能\")\n",
    "    print(f\"   - 建立人工验证流程，特别是对于高重要性项目\")\n",
    "    print(f\"   - 定期更新和优化检测模型\")\n",
    "\n",
    "provide_recommendations(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行完整分析\n",
    "def run_complete_analysis():\n",
    "    \"\"\"\n",
    "    运行完整的综合分析\n",
    "    \"\"\"\n",
    "    print(\"开始用户故事歧义检测综合分析...\")\n",
    "    print(f\"分析 {len(AMBIGUITY_TYPES)} 种歧义类型\")\n",
    "    print(f\"评估 {len(MODEL_NAMES)} 个模型\")\n",
    "    \n",
    "    # 加载结果\n",
    "    all_results = load_all_results()\n",
    "    \n",
    "    if not all_results:\n",
    "        print(\"没有找到有效的评估结果，请先运行各个歧义类型的检测notebook。\")\n",
    "        return\n",
    "    \n",
    "    # 创建汇总表\n",
    "    summary_df = create_summary_table(all_results)\n",
    "    \n",
    "    # 创建可视化\n",
    "    create_comprehensive_visualization(summary_df)\n",
    "    \n",
    "    # 分析\n",
    "    analyze_extremes(summary_df)\n",
    "    analyze_difficulty(summary_df)\n",
    "    analyze_model_strengths(summary_df)\n",
    "    \n",
    "    # 保存结果\n",
    "    save_comprehensive_results(summary_df)\n",
    "    \n",
    "    # 提供建议\n",
    "    provide_recommendations(summary_df)\n",
    "    \n",
    "    print(\"\\n综合分析完成！\")\n",
    "\n",
    "# 运行分析\n",
    "# run_complete_analysis()  # 取消注释以运行完整分析"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}